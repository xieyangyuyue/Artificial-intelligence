"""
KNN算法介绍(K Nearest Neighbors), K近邻算法
    原理:
        基于 欧式距离(或者其它距离计算方式)计算 测试集 和 每个训练集之间的距离, 然后根据距离升序排列, 找到最近的K个样本.
        基于K个样本投票, 票数多的就作为最终预测结果 -> 分类问题.
        基于K个样本计算平均值, 作为最终预测结果 -> 回归问题.
    实现思路:
        1. 分类问题
            适用于: 有特征, 有标签, 且标签是不连续的(离散的)
        2. 回归问题.
            适用于: 有特征, 有标签, 且标签是连续的.
    KNN算法, 回归问题思路如下:
        1. 计算测试集和每个训练的样本之间的 距离.
        2. 基于距离进行升序排列.
        3. 找到最近的K个样本.
        4. 基于K个样本的标签值, 计算平均值.
        5. 将上述计算出来的平均值, 作为最终的预测结果.
    代码实现思路:
        1. 导包.
        2. 准备数据集(测试集 和 训练集)
        3. 创建(KNN 回归模型)模型对象.
        4. 模型训练.
        5. 模型预测.
    总结:
        K值过小, 容易受到异常值的影响, 且会导致模型学到大量的"脏的特征", 导致出现: 过拟合.
        K值过大, 模型会变得简单, 容易发生: 欠拟合.
"""

# 1. 导包.
from sklearn.neighbors import KNeighborsRegressor       # KNN算法的 回归模型

# 2. 准备数据集(测试集 和 训练集)
# 开根号:     14.53      14.28       1           2.24
# 平方和:      211        204        1            5
# 差值:     (3,11,9)   (2,10,10)  (0,1,0)      (1,0,2)
x_train = [[0, 0, 1], [1, 1, 0], [3, 10, 10], [4, 11, 12]]      # 训练集的特征数据, 因为特征可以有多个特征, 所以是一个二维数组
y_train = [0.1, 0.2, 0.3, 0.4]                                  # 训练集的标签数据, 因为标签是连续的, 所以是一个一维数组
x_test = [[3, 11, 10]]                                          # 测试集的特征数据

# 3. 创建(KNN 回归模型)模型对象.
estimator = KNeighborsRegressor(n_neighbors=2)

# 4. 模型训练.
estimator.fit(x_train, y_train)

# 5. 模型预测.
y_pre = estimator.predict(x_test)

# 6. 打印预测结果.
print(f'预测值为: {y_pre}')