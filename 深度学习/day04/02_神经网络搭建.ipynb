{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1ad6fd-b7e4-47c3-bd02-c482751f597f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "案例:\n",
    "    演示神经网络搭建流程.\n",
    "\n",
    "深度学习案例的4个步骤:\n",
    "    1. 准备数据.\n",
    "    2. 搭建神经网络\n",
    "    3. 模型训练\n",
    "    4. 模型测试\n",
    "\n",
    "神经网络搭建流程:\n",
    "    1. 定义一个类, 继承: nn.Module\n",
    "    2. 在__init__()方法中, 搭建神经网络.\n",
    "    3. 在 forward()方法中,完成: 前向传播.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ff443-cf2d-438a-bb7f-8640c10e2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary  # 计算模型参数,查看模型结构, pip install torchsummary -i https://mirrors.aliyun.com/pypi/simple/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d443d811-309c-4db5-8481-34090062a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[ 0.3829,  0.9104,  1.3151],\n",
      "        [-0.8089,  0.7915,  1.2898],\n",
      "        [ 0.7575,  0.4327,  1.7061],\n",
      "        [ 1.2026,  0.5042,  0.1319],\n",
      "        [ 0.4193,  0.0179,  0.6583]])\n",
      "data.shape: torch.Size([5, 3])\n",
      "data.requires_grad: False\n",
      "output: tensor([[0.5054, 0.4946],\n",
      "        [0.5106, 0.4894],\n",
      "        [0.5080, 0.4920],\n",
      "        [0.5030, 0.4970],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward0>)\n",
      "output.shape: torch.Size([5, 2])\n",
      "output.requires_grad: True\n",
      "------------------------------\n",
      "=================== 计算模型参数 ===================\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 5, 3]              12\n",
      "            Linear-2                 [-1, 5, 2]               8\n",
      "            Linear-3                 [-1, 5, 2]               6\n",
      "================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "=================== 查看模型参数 ===================\n",
      "name: linear1.weight\n",
      "param: Parameter containing:\n",
      "tensor([[0.8650, 0.7211, 0.3190],\n",
      "        [0.2994, 0.1841, 0.7996],\n",
      "        [0.8344, 0.0481, 1.0940]], requires_grad=True) \n",
      "\n",
      "name: linear1.bias\n",
      "param: Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True) \n",
      "\n",
      "name: linear2.weight\n",
      "param: Parameter containing:\n",
      "tensor([[ 0.8902,  0.6108, -1.0947],\n",
      "        [ 0.3768,  0.1055, -0.3925]], requires_grad=True) \n",
      "\n",
      "name: linear2.bias\n",
      "param: Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True) \n",
      "\n",
      "name: output.weight\n",
      "param: Parameter containing:\n",
      "tensor([[ 0.3332, -0.1319],\n",
      "        [ 0.5428,  0.1533]], requires_grad=True) \n",
      "\n",
      "name: output.bias\n",
      "param: Parameter containing:\n",
      "tensor([0.3086, 0.2187], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todo: 1.搭建神经网络, 即: 自定义继承 nn.Module\n",
    "class ModelDemo(nn.Module):\n",
    "    # todo: 1.1 在init魔法方法中, 完成初始化: 父类成员, 及 神经网络搭建.\n",
    "    def __init__(self):\n",
    "        # 1.1 初始化父类成员.\n",
    "        super().__init__()\n",
    "        # 1.2 搭建神经网络 -> 隐藏层 + 输出层\n",
    "        # 隐藏层1: 输入特征数 3, 输出特征数 3\n",
    "        self.linear1 = nn.Linear(3, 3)\n",
    "        # 隐藏层2: 输入特征数 3, 输出特征数 2\n",
    "        self.linear2 = nn.Linear(3, 2)\n",
    "        # 输出层: 输入特征数 2, 输出特征数 2\n",
    "        self.output = nn.Linear(2, 2)\n",
    "\n",
    "        # 1.3 对隐藏层进行参数初始化.\n",
    "        # 隐藏层1\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "\n",
    "        # 隐藏层2\n",
    "        nn.init.kaiming_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "    # todo: 1.2 前向传播: 输入层 -> 隐藏层 -> 输出层\n",
    "    def forward(self, x):\n",
    "        # 1.1 第1层 隐藏层计算: 加权求和 + 激活函数(Sigmoid)\n",
    "        # 分解版写法.\n",
    "        # x = self.linear1(x)     # 加权求和\n",
    "        # x = torch.sigmoid(x)    # 激活函数\n",
    "\n",
    "        # 合并版写法.\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "\n",
    "        # 1.2 第2层 隐藏层计算: 加权求和 + 激活函数(ReLu)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "\n",
    "        # 1.3 第3层 输出层计算: 加权求和 + 激活函数(Softmax)\n",
    "        # dim=-1, 表示按行计算, 一条样本一条样本的处理.\n",
    "        x = torch.softmax(self.output(x), dim=-1)\n",
    "\n",
    "        # 1.4 返回预测值.\n",
    "        return x\n",
    "\n",
    "\n",
    "# todo: 2.模型训练.\n",
    "def train():\n",
    "    # 1. 创建模型对象.\n",
    "    my_model = ModelDemo()\n",
    "    # print(f'my_model: {my_model}')\n",
    "\n",
    "    # 2. 创建数据集样本, 随机生成.\n",
    "    data = torch.randn(size=(5, 3))\n",
    "    print(f'data: {data}')\n",
    "    print(f'data.shape: {data.shape}')                  # (5行, 3列)\n",
    "    print(f'data.requires_grad: {data.requires_grad}')  # False\n",
    "\n",
    "    # 3. 调用神经网络模型 -> 进行模型训练.\n",
    "    output = my_model(data)     # 底层自动调用了 forward()方法, 进行 前向传播.\n",
    "    print(f'output: {output}')\n",
    "    print(f'output.shape: {output.shape}')                  # (5行, 2列)\n",
    "    print(f'output.requires_grad: {output.requires_grad}')  # True\n",
    "    print('-' * 30)\n",
    "\n",
    "    # 4. 计算 和 查看模型参数.\n",
    "    print('=================== 计算模型参数 ===================')\n",
    "    # 参1: (神经网络)模型对象, 参2: 输入数据维度(5行3列)\n",
    "    summary(my_model, input_size=(5, 3))\n",
    "\n",
    "    print('=================== 查看模型参数 ===================')\n",
    "    for name, param in my_model.named_parameters():\n",
    "        print(f'name: {name}')\n",
    "        print(f'param: {param} \\n')\n",
    "\n",
    "\n",
    "\n",
    "# todo: 3.测试\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
