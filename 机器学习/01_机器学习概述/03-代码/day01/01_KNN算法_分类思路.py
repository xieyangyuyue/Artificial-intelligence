"""
KNN算法介绍(K Nearest Neighbors), K近邻算法
    原理:
        基于 欧式距离(或者其它距离计算方式)计算 测试集 和 每个训练集之间的距离, 然后根据距离升序排列, 找到最近的K个样本.
        基于K个样本投票, 票数多的就作为最终预测结果 -> 分类问题.
        基于K个样本计算平均值, 作为最终预测结果 -> 回归问题.
    实现思路:
        1. 分类问题
            适用于: 有特征, 有标签, 且标签是不连续的(离散的)
        2. 回归问题.
            适用于: 有特征, 有标签, 且标签是连续的.
    KNN算法, 分类问题思路如下:
        1. 计算测试集和每个训练的样本之间的 距离.
        2. 基于距离进行升序排列.
        3. 找到最近的K个样本.
        4. K个样本进行投票.
        5. 票数多的结果, 作为最终的预测结果.
    代码实现思路:
        1. 导包.
        2. 准备数据集(测试集 和 训练集)
        3. 创建(KNN 分类模型)模型对象.
        4. 模型训练.
        5. 模型预测.
"""

# 1. 导包.
from sklearn.neighbors import KNeighborsClassifier  # 分类
# from sklearn.neighbors import KNeighborsRegressor   # 回归
# 解释: 从scikit-learn库的neighbors模块导入KNN算法类
# KNeighborsClassifier: 用于分类问题的K近邻算法
# KNeighborsRegressor: 用于回归问题的K近邻算法(被注释掉，表示本次只使用分类)
# scikit-learn是Python中最常用的机器学习库，提供了各种算法的实现

# 2. 准备数据集(测试集 和 训练集)
# train: 训练集
# test: 测试集
# neighbors: 最近邻的邻居数
x_train = [[0], [1], [2], [3]]      # 训练集的特征数据, 因为特征可以有多个特征, 所以是一个二维数组
y_train = [0, 0, 1, 1]              # 训练集的标签数据, 因为标签是离散的, 所以是一个一维数组
x_test = [[5]]                      # 测试集的特征数据
# 解释: 准备训练数据和测试数据
# x_train: 训练集特征，包含4个样本，每个样本只有1个特征值
#   - 样本1: 特征值为0
#   - 样本2: 特征值为1  
#   - 样本3: 特征值为2
#   - 样本4: 特征值为3
# y_train: 训练集标签，对应4个样本的分类标签
#   - 样本1和2属于类别0
#   - 样本3和4属于类别1
# x_test: 测试集特征，包含1个样本，特征值为5
# 注意: 这是一个简化的示例，实际应用中特征通常是多维的

# 3. 创建(KNN 分类模型)模型对象.
# estimator: 估计器, 模型对象, 也可以用变量名 model做接收.
estimator = KNeighborsClassifier(n_neighbors=3)
# 解释: 创建KNN分类器实例
# KNeighborsClassifier(): 创建KNN分类器对象
# n_neighbors=3: 设置K值为3，即考虑最近的3个邻居进行投票
# estimator: 变量名，在机器学习中常用来指代模型对象
# 其他重要参数(默认值):
#   weights: 权重函数，'uniform'(均匀权重)或'distance'(距离权重)
#   algorithm: 计算最近邻的算法，如'auto', 'ball_tree', 'kd_tree', 'brute'
#   metric: 距离度量方式，默认'minkowski'(闵可夫斯基距离)

# 4. 模型训练
# 传入: 训练集的特征数据, 训练集的标签数据
estimator.fit(x_train, y_train)
# 解释: 使用训练数据拟合模型
# fit()方法: 训练模型的方法
# x_train: 训练特征数据
# y_train: 训练标签数据
# 对于KNN算法，训练过程实际上只是存储训练数据，不进行复杂的计算
# 真正的计算发生在预测阶段，当需要预测新样本时才会计算距离

# 5. 模型预测.
# 传入: 测试集的特征数据, 获取到: 预测结果(测试集的标签, y_test)
y_pre = estimator.predict(x_test)
# 解释: 使用训练好的模型进行预测
# predict()方法: 对测试数据进行预测
# x_test: 测试特征数据
# y_pre: 预测结果，即模型认为测试样本属于哪个类别
# 具体计算过程:
#   1. 计算测试样本[5]与所有训练样本的距离:
#      - 与[0]的距离: |5-0| = 5
#      - 与[1]的距离: |5-1| = 4  
#      - 与[2]的距离: |5-2| = 3
#      - 与[3]的距离: |5-3| = 2
#   2. 按距离排序: [3](距离2), [2](距离3), [1](距离4), [0](距离5)
#   3. 取最近的3个邻居(K=3): [3], [2], [1]
#   4. 邻居标签: [3]→1, [2]→1, [1]→0
#   5. 投票: 类别1有2票，类别0有1票
#   6. 预测结果: 类别1

# 6. 打印预测结果.
print(f'预测值为: {y_pre}')
# 解释: 输出预测结果
# 使用f-string格式化输出，使结果更清晰
# 预期输出: 预测值为: [1]
# 因为测试样本[5]距离训练样本[3]最近，而[3]属于类别1