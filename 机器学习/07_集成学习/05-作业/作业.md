## 集成学习的方式

1. bagging
   - 样本有放回地随机采样
   - 多个基模型同时训练
   - 每个模型平等投票,多数作为预测值
2. boosting
   - 样本不进行采样,为每个样本赋值初始权重
   - 预测错误样本权重增加,预测正确样本权重减小，根据错误率为模型分配权重，再次训练模型
   - 重复上述步骤
   - 模型的加权投票作为预测值

## 案例

随机森林泰坦尼克案例

```python
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier

df = pd.read_csv("D:/myAI/Resilio Sync/05-决策树/03-代码/titanic/train.csv")

X = df[["Pclass", "Sex", "Age"]].copy()
y = df["Survived"]

X["Age"] = X["Age"].fillna(X["Age"].mean())
X = pd.get_dummies(X)
X.drop("Sex_male", axis=1, inplace=True)

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# 随机森林
rf = RandomForestClassifier()
rf.fit(x_train, y_train)

print(rf.score(x_test, y_test))

# 交叉网格搜索
paras = {"n_estimators": [10, 20, 50, 100, 120], "max_depth": [2, 4, 6, 8]}
model = GridSearchCV(estimator=rf, param_grid=paras, cv=5)
model.fit(x_train, y_train)
print(model.best_estimator_)

rf_new = RandomForestClassifier(max_depth=6, n_estimators=10)
rf_new.fit(x_train, y_train)

print(rf_new.score(x_test, y_test))

# 0.7932960893854749
# RandomForestClassifier(max_depth=2, n_estimators=50)
# 0.7988826815642458
```

葡萄酒案例

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

df = pd.read_csv("D:/myAI/Resilio Sync/06-集成学习/03-代码/data/wine0501.csv")
print(df.info())
print(df.head())

# 特征预处理，有三类，去掉1类
df = df[df["Class label"] != 1]
X = df[["Alcohol", "Hue"]]
y = df["Class label"]

# 类别转化
y = LabelEncoder().fit_transform(y)

# 划分数据集
x_train, x_test, y_train, y_test = train_test_split(X, y)

# 实例化
mytree = DecisionTreeClassifier(criterion="entropy", max_depth=1, random_state=42)
model = AdaBoostClassifier(base_estimator=mytree, n_estimators=500, learning_rate=0.1)
# model = AdaBoostClassifier()
# 训练模型
model.fit(x_train, y_train)
print(model.score(x_test, y_test))

# 0.9333333333333333
```

GBDT泰坦尼克案例

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv("D:/myAI/Resilio Sync/05-决策树/03-代码/titanic/train.csv")

X = df[["Pclass", "Sex", "Age"]].copy()
y = df["Survived"]

X["Age"] = X["Age"].fillna(X["Age"].mean())
X = pd.get_dummies(X)
X.drop("Sex_male", axis=1, inplace=True)

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

estimator = GradientBoostingClassifier(n_estimators=500)
estimator.fit(x_train, y_train)
print(estimator.score(x_test, y_test))

# 0.7821229050279329
```

XGBoost红酒品质分类

```python
import pandas as pd
from xgboost import XGBClassifier
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.utils import class_weight
from sklearn.model_selection import StratifiedKFold, GridSearchCV


def save_file():
    # 加载数据集
    df = pd.read_csv("D:/myAI/Resilio Sync/06-集成学习/03-代码/data/红酒品质分类.csv")
    x = df.iloc[:, :-1]
    y = df.iloc[:, -1] - 3
    # 数据集划分
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y)
    # 数据存储
    pd.concat([x_train, y_train], axis=1).to_csv("./data/红酒品质分类-train.csv")
    pd.concat([x_test, y_test], axis=1).to_csv("./data/红酒品质分类-test.csv")


# 加载数据集
def model_train():
    data_train = pd.read_csv("data/红酒品质分类-train.csv")
    data_test = pd.read_csv("data/红酒品质分类-test.csv")
    x_train = data_train.iloc[:, :-1]
    y_train = data_train.iloc[:, -1]
    x_test = data_test.iloc[:, :-1]
    y_test = data_test.iloc[:, -1]

    # 样本不均衡为题处理
    classes_weights = class_weight.compute_sample_weight(class_weight="balanced", y=y_train)
    # 模型训练
    estimator = XGBClassifier(n_estimators=100, objective="multi:softmax",
                              eval_metric="merror", eta=0.1, use_label_encoder=False,
                              random_state=22)
    estimator.fit(x_train, y_train, classes_weights)
    # 模型评估
    y_pred = estimator.predict(x_test)
    print(classification_report(y_test, y_pred))


def gridSearch():
    # 获取数据/数据划分
    data_train = pd.read_csv("data/红酒品质分类-train.csv")
    # data_test = pd.read_csv("data/红酒品质分类-test.csv")
    x_train = data_train.iloc[:, :-1]
    y_train = data_train.iloc[:, -1]
    # x_test = data_test.iloc[:, :-1]
    # y_test = data_test.iloc[:, -1]
    # 交叉验证分层抽取
    spliter = StratifiedKFold(n_splits=5, shuffle=True)
    # 设定超参数
    param_grid = {"max_depth": np.arange(3, 5, 1),
                  "n_estimators": np.arange(50, 150, 50),
                  "eta": np.arange(0.1, 1, 0.3)}
    # 实例化
    estimator = XGBClassifier()
    estimator = GridSearchCV(estimator=estimator,
                             param_grid=param_grid, cv=spliter)
    # 模型训练
    estimator.fit(x_train, y_train)
    print(estimator.best_estimator_)


if __name__ == '__main__':
    model_train()
    gridSearch()

#              precision    recall  f1-score   support
# 
#            0       1.00      0.50      0.67         2
#            1       0.00      0.00      0.00        11
#            2       0.73      0.79      0.76       136
#            3       0.69      0.68      0.69       128
#            4       0.64      0.62      0.63        40
#            5       0.00      0.00      0.00         3
# 
#     accuracy                           0.69       320
#    macro avg       0.51      0.43      0.46       320
# weighted avg       0.67      0.69      0.68       320
# 
# XGBClassifier(base_score=None, booster=None, callbacks=None,
#               colsample_bylevel=None, colsample_bynode=None,
#               colsample_bytree=None, device=None, early_stopping_rounds=None,
#               enable_categorical=False, eta=0.4, eval_metric=None,
#               feature_types=None, gamma=None, grow_policy=None,
#               importance_type=None, interaction_constraints=None,
#               learning_rate=None, max_bin=None, max_cat_threshold=None,
#               max_cat_to_onehot=None, max_delta_step=None, max_depth=4,
#               max_leaves=None, min_child_weight=None, missing=nan,
#               monotone_constraints=None, multi_strategy=None, n_estimators=100,
#               n_jobs=None, num_parallel_tree=None, ...)
```

